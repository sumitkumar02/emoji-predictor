{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emoji .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj_o5H2dXgKh",
        "colab_type": "code",
        "outputId": "8183006e-faf2-4506-d127-4a558c0dd645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "source": [
        "! pip install twitter\n",
        "! pip install emoji\n",
        "! pip install nb_utils\n",
        "! pip install gensim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting twitter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/e2/f602e3f584503f03e0389491b251464f8ecfe2596ac86e6b9068fe7419d3/twitter-1.18.0-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 2.4MB/s \n",
            "\u001b[?25hInstalling collected packages: twitter\n",
            "Successfully installed twitter-1.18.0\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/ee/8cc0af26113508c8513dac40b1990b21d1d0136b3981a8b7b8a231a56c8d/emoji-0.5.2-py3-none-any.whl (41kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51kB 3.9MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-0.5.2\n",
            "Collecting nb_utils\n",
            "\u001b[31m  ERROR: Could not find a version that satisfies the requirement nb_utils (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for nb_utils\u001b[0m\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.2.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.139)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.2)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.3.9)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.0)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.139 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.139)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.139->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.139->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4P91p4oXUpD",
        "colab_type": "code",
        "outputId": "0a12cb37-9c99-4841-cb4e-ebccbd48fe86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import random\n",
        "import twitter\n",
        "import emoji\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from itertools import chain\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import one_hot\n",
        "import keras.callbacks\n",
        "import json\n",
        "\n",
        "import os\n",
        "#import nb_utils\n",
        "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, Embedding, GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.layers import Concatenate, Average\n",
        "\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzCXYWkEXUpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fill these in!\n",
        "\n",
        "CONSUMER_KEY = 'xbMuxcJpRTiVGt2C2EYnA'\n",
        "CONSUMER_SECRET = '2DbQTsvIptkPTdaUcos8DDvQH9fzO0hNjJpUT2uVzQ'\n",
        "ACCESS_TOKEN = '7319442-EDm4CPxL7W4KkZcGWRMJNVHp88W5OH9vgblu898fg'\n",
        "ACCESS_SECRET = '5ZxJSbqXhG7uhgXzTFWf9XhkfsxxinlPRXyDTzbA9w'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owsk286DXUpV",
        "colab_type": "code",
        "outputId": "5ec19bd0-88ae-4e81-e368-d254bdb9e96b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "auth=twitter.OAuth(\n",
        "    consumer_key=CONSUMER_KEY,\n",
        "    consumer_secret=CONSUMER_SECRET,\n",
        "    token=ACCESS_TOKEN,\n",
        "    token_secret=ACCESS_SECRET,\n",
        ")\n",
        "\n",
        "status_stream = twitter.TwitterStream(auth=auth).statuses\n",
        "\n",
        "[x['text'] for x in itertools.islice(status_stream.sample(), 0, 5) if x.get('text')]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"RT @bayramsenocak: Cumhurba≈ükanlƒ±ƒüƒ± G√ºvenlik ve Dƒ±≈ü Politikalar Kurulu √úyesi Prof. Dr. Beril Dedeoƒülu hocamƒ±za Allah'tan rahmet, ailesine v‚Ä¶\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj90BI7eXUpe",
        "colab_type": "code",
        "outputId": "3f036ad4-b170-441c-f354-872392dbc6ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "status_stream = twitter.TwitterStream(auth=auth).statuses\n",
        "\n",
        "def english_has_emoji(tweet):\n",
        "    if tweet.get('lang') != 'en':\n",
        "        return False\n",
        "    return any(ch for ch in tweet.get('text', '') if ch in emoji.UNICODE_EMOJI)\n",
        "\n",
        "%time tweets = list(itertools.islice(filter(english_has_emoji, status_stream.sample()), 0, 100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.42 s, sys: 189 ms, total: 1.61 s\n",
            "Wall time: 1min 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeI0UP-pXUp2",
        "colab_type": "code",
        "outputId": "c70c6fda-d644-4afc-b208-0c752551e33e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "stripped = []\n",
        "for tweet in tweets:\n",
        "    text = tweet['text']\n",
        "    emojis = {ch for ch in text if ch in emoji.UNICODE_EMOJI}\n",
        "    if len(emojis) == 1:\n",
        "        emoiji = emojis.pop()\n",
        "        text = ''.join(ch for ch in text if ch != emoiji)\n",
        "        stripped.append((text, emoiji))\n",
        "len(stripped)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMum7PM5Y83L",
        "colab_type": "code",
        "outputId": "8a13e0b6-f73a-40f7-e278-2401f50a0add",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFNVDl78XUqB",
        "colab_type": "code",
        "outputId": "763fe872-05cf-4e8a-f595-db82de6b5090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1058
        }
      },
      "source": [
        "\n",
        "all_tweets = pd.read_csv('/content/gdrive/My Drive/emojis.csv')\n",
        "all_tweets['emoji'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "üòÇ    124823\n",
              "‚ù§     43218\n",
              "üòç     40566\n",
              "üò≠     35714\n",
              "üòä     20076\n",
              "üôÑ     17963\n",
              "üò©     16232\n",
              "üî•     15453\n",
              "ü§î     15419\n",
              "üíï     12026\n",
              "üíØ     11783\n",
              "üòò     11065\n",
              "üíÄ      9928\n",
              "‚ú®      9886\n",
              "üôÉ      9405\n",
              "üëÄ      7842\n",
              "üòí      7019\n",
              "‚ò∫      6871\n",
              "üò¢      6846\n",
              "üò≥      6716\n",
              "üíô      6616\n",
              "üòé      6349\n",
              "üòâ      6272\n",
              "üòÖ      6133\n",
              "üòÅ      6010\n",
              "üòå      5759\n",
              "üòè      5623\n",
              "üíñ      5331\n",
              "üòî      5244\n",
              "üò¥      4999\n",
              "      ...  \n",
              "üïç         1\n",
              "üï£         1\n",
              "üàπ         1\n",
              "üì≥         1\n",
              "üè∫         1\n",
              "üìá         1\n",
              "üàö         1\n",
              "ü¶à         1\n",
              "ü§°         1\n",
              "üè£         1\n",
              "üéö         1\n",
              "üîÄ         1\n",
              "üèó         1\n",
              "ü¶ç         1\n",
              "ü•ú         1\n",
              "üì≠         1\n",
              "üè¨         1\n",
              "„äô         1\n",
              "üè§         1\n",
              "ü§†         1\n",
              "üéΩ         1\n",
              "üìê         1\n",
              "üîè         1\n",
              "üóÇ         1\n",
              "üö∏         1\n",
              "üéë         1\n",
              "üìØ         1\n",
              "üìÇ         1\n",
              "üî£         1\n",
              "üè¶         1\n",
              "Name: emoji, Length: 989, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-9XlR0YXUqJ",
        "colab_type": "code",
        "outputId": "f14f323c-1e72-4b14-acef-c90c4692f899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1058
        }
      },
      "source": [
        "tweets = all_tweets.groupby('emoji').filter(lambda c:len(c) > 1000)\n",
        "tweets['emoji'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "üòÇ    124823\n",
              "‚ù§     43218\n",
              "üòç     40566\n",
              "üò≠     35714\n",
              "üòä     20076\n",
              "üôÑ     17963\n",
              "üò©     16232\n",
              "üî•     15453\n",
              "ü§î     15419\n",
              "üíï     12026\n",
              "üíØ     11783\n",
              "üòò     11065\n",
              "üíÄ      9928\n",
              "‚ú®      9886\n",
              "üôÉ      9405\n",
              "üëÄ      7842\n",
              "üòí      7019\n",
              "‚ò∫      6871\n",
              "üò¢      6846\n",
              "üò≥      6716\n",
              "üíô      6616\n",
              "üòé      6349\n",
              "üòâ      6272\n",
              "üòÖ      6133\n",
              "üòÅ      6010\n",
              "üòå      5759\n",
              "üòè      5623\n",
              "üíñ      5331\n",
              "üòî      5244\n",
              "üò¥      4999\n",
              "      ...  \n",
              "‚úå      1523\n",
              "üì∏      1496\n",
              "üé§      1487\n",
              "üåö      1452\n",
              "üëÖ      1431\n",
              "üèà      1373\n",
              "üåü      1355\n",
              "‚è©      1332\n",
              "‚ùó      1325\n",
              "üî¥      1304\n",
              "‚òï      1296\n",
              "üëä      1273\n",
              "üëá      1259\n",
              "‚ù£      1254\n",
              "üéß      1246\n",
              "üéà      1210\n",
              "‚è≠      1198\n",
              "üí´      1181\n",
              "‚Ü™      1157\n",
              "ü§ë      1152\n",
              "‚öΩ      1141\n",
              "üòπ      1112\n",
              "üò∂      1108\n",
              "üí¶      1075\n",
              "üò£      1074\n",
              "üò•      1072\n",
              "üôÅ      1066\n",
              "ü§ï      1065\n",
              "‚òÄ      1013\n",
              "üò∞      1013\n",
              "Name: emoji, Length: 121, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5WdoaKwXUqd",
        "colab_type": "code",
        "outputId": "d7de046e-344a-465f-f6ea-8dab4775ee7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max(tweets['text'], key=lambda t:len(t))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Don't worry, my love, I don't get you wrong Don't get me wrong. We're connected with our hearts, souls, minds, bodies. If any doubt, we ask\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dhhm6CDFXUqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = list(sorted(set(chain(*tweets['text']))))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "max_sequence_len = max(len(x) for x in tweets['text'])\n",
        "\n",
        "emojis = list(sorted(set(tweets['emoji'])))\n",
        "emoji_to_idx = {em: idx for idx, em in enumerate(emojis)}\n",
        "emojis[:10]\n",
        "\n",
        "train_tweets, test_tweets = train_test_split(tweets, test_size=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjDIUaeIXUqt",
        "colab_type": "code",
        "outputId": "a4d92edd-4f04-4253-d387-bffefe5456dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        }
      },
      "source": [
        "def data_generator(tweets, batch_size):\n",
        "    while True:\n",
        "        if batch_size is None:\n",
        "            batch = tweets\n",
        "            batch_size = batch.shape[0]\n",
        "        else:\n",
        "            batch = tweets.sample(batch_size)\n",
        "        X = np.zeros((batch_size, max_sequence_len, len(chars)))\n",
        "        y = np.zeros((batch_size,))\n",
        "        for row_idx, (_, row) in enumerate(batch.iterrows()):\n",
        "            y[row_idx] = emoji_to_idx[row['emoji']]\n",
        "            for ch_idx, ch in enumerate(row['text']):\n",
        "                X[row_idx, ch_idx, char_to_idx[ch]] = 1\n",
        "        yield X, y\n",
        "\n",
        "next(data_generator(tweets, 10))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.]]]),\n",
              " array([112.,  20.,   9.,  65.,  92.,  27.,  65.,  41.,  65.,  65.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk02o_BJXUsT",
        "colab_type": "text"
      },
      "source": [
        "## Featurizing and preparing our data\n",
        "\n",
        "Just like we did when computing word embeddings, we want to featurize our data so we can classify it effectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbAn-oEdXUsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 100000\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(tweets['text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rQr40eUXUsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_tokens = tokenizer.texts_to_sequences(train_tweets['text'])\n",
        "test_tokens = tokenizer.texts_to_sequences(test_tweets['text'])\n",
        "max_num_tokens = max(len(x) for x in chain(training_tokens, test_tokens))\n",
        "training_tokens = pad_sequences(training_tokens, maxlen=max_num_tokens)\n",
        "test_tokens = pad_sequences(test_tokens, maxlen=max_num_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzFtzg_PXUs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_labels = np.asarray([emoji_to_idx[em] for em in train_tweets['emoji']])\n",
        "test_labels = np.asarray([emoji_to_idx[em] for em in test_tweets['emoji']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94TgLdT9PEeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import twitter\n",
        "import emoji\n",
        "import gensim\n",
        "import unicodedata\n",
        "import html\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "import re\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdZJFsglQ7cN",
        "colab_type": "code",
        "outputId": "cbf2f5e4-41b2-42c5-b378-122479d1fcb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "status_stream = twitter.TwitterStream(auth=auth).statuses\n",
        "next(status_stream.sample()).keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['delete'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKcwU7DeQ-AZ",
        "colab_type": "code",
        "outputId": "edf2f70c-03e3-4ef9-e601-96dd51eedf9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "RE_URL = re.compile(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?')\n",
        "RE_WHITESPACE = re.compile(r'\\s+')\n",
        "\n",
        "def strip_accents(s):\n",
        "     return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                    if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "class TokensYielder(object):\n",
        "    def __init__(self, tweet_count, stream):\n",
        "        self.tweet_count = tweet_count\n",
        "        self.stream = stream\n",
        "\n",
        "    def __iter__(self):\n",
        "        print('!')\n",
        "        count = self.tweet_count\n",
        "        for tweet in self.stream:\n",
        "            if tweet.get('lang') != 'en':\n",
        "                continue\n",
        "            text = tweet['text']\n",
        "            text = html.unescape(text)\n",
        "            text = RE_WHITESPACE.sub(' ', text)\n",
        "            text = RE_URL.sub(' ', text)\n",
        "            text = strip_accents(text)\n",
        "            text = ''.join(ch for ch in text if ord(ch) < 128)\n",
        "            if text.startswith('RT '):\n",
        "                text = text[3:]\n",
        "            text = text.strip()\n",
        "            if text:\n",
        "                yield text_to_word_sequence(text)\n",
        "                count -= 1\n",
        "                if count <= 0:\n",
        "                    break\n",
        "\n",
        "for t in TokensYielder(3, twitter.TwitterStream(auth=auth).statuses.sample()):\n",
        "    print(t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\n",
            "['amadayum', 'gogobebe9966', 'we', 'stan', 'talented', 'ladies', 'the', \"album's\", 'greatttt']\n",
            "['enderski', 'redheads', 'are', 'nuts', 'badtz', 'i', 'like', 'my', 'girls', 'nuts']\n",
            "['we', 'must', 'ensure', 'adequate', 'professional', 'and', 'technical', 'm', 'e', 'capacity', 'in', 'that', 'capacity', 'goes', 'down', 'to', 'the', 'district']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7zuV8xiO45G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = gensim.models.Word2Vec(tweets,workers=5,min_count=2)\n",
        "model.save('twitter_stream_w2v.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CItYA-vXUs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_weights(tokenizer):\n",
        "    model = Word2Vec.load('/content/gdrive/My Drive/glove.6B.100d.txt')\n",
        "    w2v = np.zeros((tokenizer.num_words, w2v_model.syn0.shape[1]))\n",
        "    for k, v in tokenizer.word_index.items():\n",
        "        if v >= tokenizer.num_words:\n",
        "            continue\n",
        "        if k in w2v_model:\n",
        "            w2v[v] = w2v_model[k]\n",
        "    return w2v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QH_8uo-XUtC",
        "colab_type": "code",
        "outputId": "53a8de09-2bd8-4886-875c-5fbb1566ca2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "\n",
        "#w2v = load_weights(tokenizer)\n",
        "\n",
        "model = Word2Vec.load('twitter_stream_w2v.model')\n",
        "w2v = np.zeros((tokenizer.num_words, model.wv.syn0.shape[1]))\n",
        "found = 0\n",
        "for k, v in tokenizer.word_index.items():\n",
        "    if v >= tokenizer.num_words:\n",
        "        continue\n",
        "    if k in model:\n",
        "        w2v[v] = model[k]\n",
        "        found += 1\n",
        "found, tokenizer.num_words\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 100000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLU9njKZXUtK",
        "colab_type": "text"
      },
      "source": [
        "# World Level CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUbk9SS0XUtM",
        "colab_type": "code",
        "outputId": "a3ad6689-19eb-42ed-e515-c7f673849833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        }
      },
      "source": [
        "def create_cnn_model(vocab_size, embedding_size=None, embedding_weights=None, drop_out=0.2):\n",
        "    message = Input(shape=(max_num_tokens,), dtype='int32', name='cnn_input')\n",
        "    \n",
        "    \n",
        "    # The convolution layer in keras does not support masking, so we just allow\n",
        "    # the embedding layer to learn an explicit value.\n",
        "    embedding = Embedding(mask_zero=False, input_dim=vocab_size, \n",
        "                          output_dim=embedding_weights.shape[1], \n",
        "                          weights=[embedding_weights],\n",
        "                          trainable=True,\n",
        "                          name='cnn_embedding')(message)\n",
        "    \n",
        "    global_pools = []\n",
        "    for window in 2, 3:\n",
        "        conv_1x = Conv1D(128, window, activation='relu', padding='valid')(embedding)\n",
        "        max_pool_1x = MaxPooling1D(2)(conv_1x)\n",
        "        conv_2x = Conv1D(256, window, activation='relu', padding='valid')(max_pool_1x)\n",
        "        max_pool_2x = MaxPooling1D(2)(conv_2x)\n",
        "        conv_3x = Conv1D(256, window, activation='relu', padding='valid')(max_pool_2x)\n",
        "\n",
        "        global_pools.append(GlobalMaxPooling1D()(conv_3x))\n",
        "\n",
        "    merged = Concatenate(axis=1)(global_pools)\n",
        "    fc1 = Dense(units=128, activation='relu')(merged)\n",
        "    preds = Dense(units=len(emojis), activation='softmax', name='cnn_predictions')(fc1)\n",
        "    model = Model(\n",
        "        inputs=[message],\n",
        "        outputs=[preds],\n",
        "    )\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "cnn_model = create_cnn_model(VOCAB_SIZE, embedding_weights=w2v)\n",
        "cnn_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cnn_input (InputLayer)          (None, 54)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "cnn_embedding (Embedding)       (None, 54, 100)      10000000    cnn_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 53, 128)      25728       cnn_embedding[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 52, 128)      38528       cnn_embedding[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 26, 128)      0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 26, 128)      0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 25, 256)      65792       max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 24, 256)      98560       max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 12, 256)      0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 12, 256)      0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 11, 256)      131328      max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 10, 256)      196864      max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 256)          0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 256)          0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 512)          0           global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_max_pooling1d_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          65664       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "cnn_predictions (Dense)         (None, 121)          15609       dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 10,638,073\n",
            "Trainable params: 10,638,073\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSxYykTIXUtW",
        "colab_type": "code",
        "outputId": "1fd47b57-d53f-42e4-a9b5-f1e5f5a4944a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6754
        }
      },
      "source": [
        "cnn_model.fit(training_tokens, training_labels, epochs=200,batch_size=10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "634695/634695 [==============================] - 63s 99us/step - loss: 3.5363 - acc: 0.2305\n",
            "Epoch 2/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 3.2956 - acc: 0.2645\n",
            "Epoch 3/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 3.0712 - acc: 0.3065\n",
            "Epoch 4/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 2.8554 - acc: 0.3509\n",
            "Epoch 5/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 2.6607 - acc: 0.3922\n",
            "Epoch 6/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 2.4904 - acc: 0.4284\n",
            "Epoch 7/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 2.3389 - acc: 0.4609\n",
            "Epoch 8/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 2.2057 - acc: 0.4898\n",
            "Epoch 9/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 2.0919 - acc: 0.5141\n",
            "Epoch 10/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.9804 - acc: 0.5381\n",
            "Epoch 11/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.8947 - acc: 0.5567\n",
            "Epoch 12/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.8028 - acc: 0.5770\n",
            "Epoch 13/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.7324 - acc: 0.5921\n",
            "Epoch 14/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.6607 - acc: 0.6078\n",
            "Epoch 15/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.6066 - acc: 0.6193\n",
            "Epoch 16/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.5396 - acc: 0.6346\n",
            "Epoch 17/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.7050 - acc: 0.6018\n",
            "Epoch 18/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.4753 - acc: 0.6486\n",
            "Epoch 19/200\n",
            "634695/634695 [==============================] - 59s 92us/step - loss: 1.4063 - acc: 0.6652\n",
            "Epoch 20/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.3623 - acc: 0.6746\n",
            "Epoch 21/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.3331 - acc: 0.6806\n",
            "Epoch 22/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.4459 - acc: 0.6621\n",
            "Epoch 23/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.3751 - acc: 0.6684\n",
            "Epoch 24/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.2554 - acc: 0.6979\n",
            "Epoch 25/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.2033 - acc: 0.7108\n",
            "Epoch 26/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.3534 - acc: 0.6858\n",
            "Epoch 27/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.2722 - acc: 0.6916\n",
            "Epoch 28/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.1605 - acc: 0.7195\n",
            "Epoch 29/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.1098 - acc: 0.7322\n",
            "Epoch 30/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.0850 - acc: 0.7380\n",
            "Epoch 31/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.2751 - acc: 0.7043\n",
            "Epoch 32/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.1652 - acc: 0.7154\n",
            "Epoch 33/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.0735 - acc: 0.7392\n",
            "Epoch 34/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.0233 - acc: 0.7517\n",
            "Epoch 35/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.9922 - acc: 0.7591\n",
            "Epoch 36/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.9736 - acc: 0.7633\n",
            "Epoch 37/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.9600 - acc: 0.7659\n",
            "Epoch 38/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.1729 - acc: 0.7269\n",
            "Epoch 39/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.1090 - acc: 0.7270\n",
            "Epoch 40/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.0094 - acc: 0.7524\n",
            "Epoch 41/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.9420 - acc: 0.7704\n",
            "Epoch 42/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.8997 - acc: 0.7812\n",
            "Epoch 43/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.8719 - acc: 0.7877\n",
            "Epoch 44/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.8505 - acc: 0.7927\n",
            "Epoch 45/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.1536 - acc: 0.7448\n",
            "Epoch 46/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.1768 - acc: 0.7096\n",
            "Epoch 47/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.9545 - acc: 0.7650\n",
            "Epoch 48/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.8750 - acc: 0.7860\n",
            "Epoch 49/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.8281 - acc: 0.7984\n",
            "Epoch 50/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.7969 - acc: 0.8061\n",
            "Epoch 51/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.7742 - acc: 0.8117\n",
            "Epoch 52/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.7582 - acc: 0.8156\n",
            "Epoch 53/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.7485 - acc: 0.8170\n",
            "Epoch 54/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.0449 - acc: 0.7663\n",
            "Epoch 55/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.9563 - acc: 0.7603\n",
            "Epoch 56/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.8475 - acc: 0.7891\n",
            "Epoch 57/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.7843 - acc: 0.8062\n",
            "Epoch 58/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.7400 - acc: 0.8185\n",
            "Epoch 59/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.7112 - acc: 0.8264\n",
            "Epoch 60/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.6898 - acc: 0.8319\n",
            "Epoch 61/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.6726 - acc: 0.8361\n",
            "Epoch 62/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.6615 - acc: 0.8382\n",
            "Epoch 63/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.8910 - acc: 0.8066\n",
            "Epoch 64/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.0401 - acc: 0.7411\n",
            "Epoch 65/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.8413 - acc: 0.7897\n",
            "Epoch 66/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.7463 - acc: 0.8144\n",
            "Epoch 67/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.6896 - acc: 0.8302\n",
            "Epoch 68/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.6520 - acc: 0.8408\n",
            "Epoch 69/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.6267 - acc: 0.8477\n",
            "Epoch 70/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.6090 - acc: 0.8524\n",
            "Epoch 71/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5951 - acc: 0.8558\n",
            "Epoch 72/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5845 - acc: 0.8580\n",
            "Epoch 73/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5766 - acc: 0.8598\n",
            "Epoch 74/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5731 - acc: 0.8601\n",
            "Epoch 75/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.6275 - acc: 0.8517\n",
            "Epoch 76/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 1.1877 - acc: 0.7237\n",
            "Epoch 77/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.8383 - acc: 0.7909\n",
            "Epoch 78/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.7171 - acc: 0.8203\n",
            "Epoch 79/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.6423 - acc: 0.8413\n",
            "Epoch 80/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5960 - acc: 0.8548\n",
            "Epoch 81/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5660 - acc: 0.8633\n",
            "Epoch 82/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5467 - acc: 0.8682\n",
            "Epoch 83/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5321 - acc: 0.8720\n",
            "Epoch 84/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5215 - acc: 0.8745\n",
            "Epoch 85/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5135 - acc: 0.8761\n",
            "Epoch 86/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5079 - acc: 0.8775\n",
            "Epoch 87/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5051 - acc: 0.8778\n",
            "Epoch 88/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5062 - acc: 0.8767\n",
            "Epoch 89/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5128 - acc: 0.8740\n",
            "Epoch 90/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.8422 - acc: 0.8209\n",
            "Epoch 91/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.9529 - acc: 0.7665\n",
            "Epoch 92/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.7695 - acc: 0.8067\n",
            "Epoch 93/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.6548 - acc: 0.8351\n",
            "Epoch 94/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5821 - acc: 0.8553\n",
            "Epoch 95/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5377 - acc: 0.8679\n",
            "Epoch 96/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5082 - acc: 0.8768\n",
            "Epoch 97/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4874 - acc: 0.8825\n",
            "Epoch 98/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4731 - acc: 0.8864\n",
            "Epoch 99/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4626 - acc: 0.8891\n",
            "Epoch 100/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4555 - acc: 0.8906\n",
            "Epoch 101/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4495 - acc: 0.8920\n",
            "Epoch 102/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4453 - acc: 0.8927\n",
            "Epoch 103/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4439 - acc: 0.8928\n",
            "Epoch 104/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4449 - acc: 0.8920\n",
            "Epoch 105/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4552 - acc: 0.8883\n",
            "Epoch 106/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.8089 - acc: 0.8335\n",
            "Epoch 107/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.8917 - acc: 0.7808\n",
            "Epoch 108/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.7239 - acc: 0.8179\n",
            "Epoch 109/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.6183 - acc: 0.8438\n",
            "Epoch 110/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5475 - acc: 0.8633\n",
            "Epoch 111/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5020 - acc: 0.8766\n",
            "Epoch 112/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4718 - acc: 0.8858\n",
            "Epoch 113/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4510 - acc: 0.8915\n",
            "Epoch 114/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4367 - acc: 0.8956\n",
            "Epoch 115/200\n",
            "634695/634695 [==============================] - 59s 92us/step - loss: 0.4263 - acc: 0.8983\n",
            "Epoch 116/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4188 - acc: 0.8999\n",
            "Epoch 117/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4136 - acc: 0.9013\n",
            "Epoch 118/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4096 - acc: 0.9022\n",
            "Epoch 119/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4070 - acc: 0.9025\n",
            "Epoch 120/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4073 - acc: 0.9023\n",
            "Epoch 121/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4094 - acc: 0.9011\n",
            "Epoch 122/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4230 - acc: 0.8963\n",
            "Epoch 123/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.6305 - acc: 0.8653\n",
            "Epoch 124/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.8087 - acc: 0.8008\n",
            "Epoch 125/200\n",
            "634695/634695 [==============================] - 59s 92us/step - loss: 0.6920 - acc: 0.8271\n",
            "Epoch 126/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5946 - acc: 0.8496\n",
            "Epoch 127/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5266 - acc: 0.8678\n",
            "Epoch 128/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4769 - acc: 0.8816\n",
            "Epoch 129/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4442 - acc: 0.8910\n",
            "Epoch 130/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4228 - acc: 0.8973\n",
            "Epoch 131/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4072 - acc: 0.9020\n",
            "Epoch 132/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.3954 - acc: 0.9051\n",
            "Epoch 133/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.3876 - acc: 0.9071\n",
            "Epoch 134/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.3816 - acc: 0.9087\n",
            "Epoch 135/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.3767 - acc: 0.9099\n",
            "Epoch 136/200\n",
            "634695/634695 [==============================] - 59s 92us/step - loss: 0.3734 - acc: 0.9105\n",
            "Epoch 137/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.3710 - acc: 0.9109\n",
            "Epoch 138/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.3701 - acc: 0.9110\n",
            "Epoch 139/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.3723 - acc: 0.9102\n",
            "Epoch 140/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.3801 - acc: 0.9073\n",
            "Epoch 141/200\n",
            "634695/634695 [==============================] - 59s 92us/step - loss: 0.4020 - acc: 0.9001\n",
            "Epoch 142/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.7394 - acc: 0.8408\n",
            "Epoch 143/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.7961 - acc: 0.8066\n",
            "Epoch 144/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.6657 - acc: 0.8335\n",
            "Epoch 145/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.5628 - acc: 0.8573\n",
            "Epoch 146/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.4927 - acc: 0.8765\n",
            "Epoch 147/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.4497 - acc: 0.8884\n",
            "Epoch 148/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.4183 - acc: 0.8975\n",
            "Epoch 149/200\n",
            "634695/634695 [==============================] - 59s 92us/step - loss: 0.3968 - acc: 0.9038\n",
            "Epoch 150/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.3823 - acc: 0.9082\n",
            "Epoch 151/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3722 - acc: 0.9108\n",
            "Epoch 152/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3648 - acc: 0.9127\n",
            "Epoch 153/200\n",
            "634695/634695 [==============================] - 59s 93us/step - loss: 0.3595 - acc: 0.9142\n",
            "Epoch 154/200\n",
            "634695/634695 [==============================] - 59s 92us/step - loss: 0.3560 - acc: 0.9148\n",
            "Epoch 155/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3530 - acc: 0.9154\n",
            "Epoch 156/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3505 - acc: 0.9160\n",
            "Epoch 157/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3501 - acc: 0.9159\n",
            "Epoch 158/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3515 - acc: 0.9153\n",
            "Epoch 159/200\n",
            "634695/634695 [==============================] - 58s 91us/step - loss: 0.3552 - acc: 0.9141\n",
            "Epoch 160/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3680 - acc: 0.9099\n",
            "Epoch 161/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.4023 - acc: 0.8987\n",
            "Epoch 162/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.6555 - acc: 0.8514\n",
            "Epoch 163/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.7362 - acc: 0.8214\n",
            "Epoch 164/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.6248 - acc: 0.8441\n",
            "Epoch 165/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.5346 - acc: 0.8646\n",
            "Epoch 166/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.4690 - acc: 0.8819\n",
            "Epoch 167/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.4255 - acc: 0.8940\n",
            "Epoch 168/200\n",
            "634695/634695 [==============================] - 58s 91us/step - loss: 0.3955 - acc: 0.9029\n",
            "Epoch 169/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3750 - acc: 0.9092\n",
            "Epoch 170/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3623 - acc: 0.9124\n",
            "Epoch 171/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3540 - acc: 0.9150\n",
            "Epoch 172/200\n",
            "634695/634695 [==============================] - 58s 91us/step - loss: 0.3473 - acc: 0.9165\n",
            "Epoch 173/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3424 - acc: 0.9176\n",
            "Epoch 174/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3392 - acc: 0.9182\n",
            "Epoch 175/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3378 - acc: 0.9187\n",
            "Epoch 176/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3369 - acc: 0.9188\n",
            "Epoch 177/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3377 - acc: 0.9183\n",
            "Epoch 178/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3405 - acc: 0.9173\n",
            "Epoch 179/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3482 - acc: 0.9144\n",
            "Epoch 180/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3625 - acc: 0.9100\n",
            "Epoch 181/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3950 - acc: 0.8998\n",
            "Epoch 182/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.4610 - acc: 0.8818\n",
            "Epoch 183/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.5296 - acc: 0.8655\n",
            "Epoch 184/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.5620 - acc: 0.8582\n",
            "Epoch 185/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.5363 - acc: 0.8645\n",
            "Epoch 186/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.4930 - acc: 0.8744\n",
            "Epoch 187/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.4463 - acc: 0.8864\n",
            "Epoch 188/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.4130 - acc: 0.8960\n",
            "Epoch 189/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3874 - acc: 0.9032\n",
            "Epoch 190/200\n",
            "634695/634695 [==============================] - 58s 91us/step - loss: 0.3714 - acc: 0.9081\n",
            "Epoch 191/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3588 - acc: 0.9117\n",
            "Epoch 192/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3505 - acc: 0.9141\n",
            "Epoch 193/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3432 - acc: 0.9158\n",
            "Epoch 194/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3376 - acc: 0.9176\n",
            "Epoch 195/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3326 - acc: 0.9188\n",
            "Epoch 196/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3291 - acc: 0.9195\n",
            "Epoch 197/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3282 - acc: 0.9198\n",
            "Epoch 198/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3274 - acc: 0.9200\n",
            "Epoch 199/200\n",
            "634695/634695 [==============================] - 58s 92us/step - loss: 0.3285 - acc: 0.9194\n",
            "Epoch 200/200\n",
            "634695/634695 [==============================] - 58s 91us/step - loss: 0.3325 - acc: 0.9179\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f62c80d3eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSAB5gkyXUtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_lstm_model(vocab_size, embedding_size=None, embedding_weights=None):\n",
        "    message = Input(shape=(None,), dtype='int32', name='lstm_input')\n",
        "    embedding = Embedding(mask_zero=False, input_dim=vocab_size, \n",
        "                          output_dim=embedding_weights.shape[1], \n",
        "                          weights=[embedding_weights],\n",
        "                          trainable=True,\n",
        "                          name='lstm_embedding')(message)\n",
        "\n",
        "    lstm_1 = LSTM(units=128, return_sequences=False)(embedding)\n",
        "    preds = Dense(units=len(emojis), activation='softmax', name='lstm_predictions')(lstm_1)\n",
        "    \n",
        "    model = Model(\n",
        "        inputs=[message],\n",
        "        outputs=[preds],\n",
        "    )\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEl5yKgfXUtj",
        "colab_type": "code",
        "outputId": "b906b480-71ce-41e9-ad26-3d8f9fb178c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "lstm_model = create_lstm_model(VOCAB_SIZE, embedding_weights=w2v)\n",
        "lstm_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_input (InputLayer)      (None, None)              0         \n",
            "_________________________________________________________________\n",
            "lstm_embedding (Embedding)   (None, None, 100)         10000000  \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               117248    \n",
            "_________________________________________________________________\n",
            "lstm_predictions (Dense)     (None, 121)               15609     \n",
            "=================================================================\n",
            "Total params: 10,132,857\n",
            "Trainable params: 10,132,857\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS7yi0p7XUtn",
        "colab_type": "code",
        "outputId": "2a02a4ad-242b-4a8f-f3fe-1b8501f13385",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10097
        }
      },
      "source": [
        "lstm_model.fit(training_tokens, training_labels, epochs=300, batch_size=10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 3.9675 - acc: 0.1828\n",
            "Epoch 2/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 3.5340 - acc: 0.2425\n",
            "Epoch 3/300\n",
            "634695/634695 [==============================] - 48s 76us/step - loss: 3.2527 - acc: 0.2846\n",
            "Epoch 4/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 2.9923 - acc: 0.3320\n",
            "Epoch 5/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 2.7958 - acc: 0.3707\n",
            "Epoch 6/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 2.5991 - acc: 0.4102\n",
            "Epoch 7/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 2.4483 - acc: 0.4411\n",
            "Epoch 8/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 2.2987 - acc: 0.4722\n",
            "Epoch 9/300\n",
            "634695/634695 [==============================] - 49s 76us/step - loss: 2.1708 - acc: 0.4972\n",
            "Epoch 10/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 2.0686 - acc: 0.5183\n",
            "Epoch 11/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.9567 - acc: 0.5408\n",
            "Epoch 12/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.8669 - acc: 0.5592\n",
            "Epoch 13/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.7898 - acc: 0.5747\n",
            "Epoch 14/300\n",
            "634695/634695 [==============================] - 49s 76us/step - loss: 1.7211 - acc: 0.5891\n",
            "Epoch 15/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.6592 - acc: 0.6027\n",
            "Epoch 16/300\n",
            "634695/634695 [==============================] - 48s 76us/step - loss: 1.6068 - acc: 0.6137\n",
            "Epoch 17/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.5536 - acc: 0.6254\n",
            "Epoch 18/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.5134 - acc: 0.6340\n",
            "Epoch 19/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.4709 - acc: 0.6430\n",
            "Epoch 20/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.4345 - acc: 0.6515\n",
            "Epoch 21/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.4042 - acc: 0.6578\n",
            "Epoch 22/300\n",
            "634695/634695 [==============================] - 48s 76us/step - loss: 1.3719 - acc: 0.6647\n",
            "Epoch 23/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.3512 - acc: 0.6692\n",
            "Epoch 24/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.3216 - acc: 0.6760\n",
            "Epoch 25/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.2940 - acc: 0.6821\n",
            "Epoch 26/300\n",
            "634695/634695 [==============================] - 48s 76us/step - loss: 1.2714 - acc: 0.6875\n",
            "Epoch 27/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.2560 - acc: 0.6900\n",
            "Epoch 28/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.2287 - acc: 0.6969\n",
            "Epoch 29/300\n",
            "634695/634695 [==============================] - 48s 76us/step - loss: 1.2077 - acc: 0.7020\n",
            "Epoch 30/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.1904 - acc: 0.7056\n",
            "Epoch 31/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.1756 - acc: 0.7089\n",
            "Epoch 32/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.1572 - acc: 0.7131\n",
            "Epoch 33/300\n",
            "634695/634695 [==============================] - 49s 76us/step - loss: 1.1450 - acc: 0.7152\n",
            "Epoch 34/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.1289 - acc: 0.7193\n",
            "Epoch 35/300\n",
            "634695/634695 [==============================] - 48s 76us/step - loss: 1.1135 - acc: 0.7230\n",
            "Epoch 36/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.0991 - acc: 0.7266\n",
            "Epoch 37/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.0849 - acc: 0.7293\n",
            "Epoch 38/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.0756 - acc: 0.7313\n",
            "Epoch 39/300\n",
            "634695/634695 [==============================] - 49s 76us/step - loss: 1.1087 - acc: 0.7224\n",
            "Epoch 40/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.1028 - acc: 0.7214\n",
            "Epoch 41/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.0585 - acc: 0.7349\n",
            "Epoch 42/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.0314 - acc: 0.7424\n",
            "Epoch 43/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 1.0129 - acc: 0.7469\n",
            "Epoch 44/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.9993 - acc: 0.7504\n",
            "Epoch 45/300\n",
            "634695/634695 [==============================] - 49s 76us/step - loss: 0.9879 - acc: 0.7530\n",
            "Epoch 46/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.9789 - acc: 0.7546\n",
            "Epoch 47/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.9699 - acc: 0.7570\n",
            "Epoch 48/300\n",
            "634695/634695 [==============================] - 48s 76us/step - loss: 0.9611 - acc: 0.7587\n",
            "Epoch 49/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.9551 - acc: 0.7601\n",
            "Epoch 50/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.9513 - acc: 0.7606\n",
            "Epoch 51/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.9441 - acc: 0.7624\n",
            "Epoch 52/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.9399 - acc: 0.7631\n",
            "Epoch 53/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.9298 - acc: 0.7656\n",
            "Epoch 54/300\n",
            "634695/634695 [==============================] - 49s 76us/step - loss: 0.9252 - acc: 0.7666\n",
            "Epoch 55/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.9166 - acc: 0.7688\n",
            "Epoch 56/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.9072 - acc: 0.7710\n",
            "Epoch 57/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.8988 - acc: 0.7732\n",
            "Epoch 58/300\n",
            "634695/634695 [==============================] - 49s 76us/step - loss: 0.8900 - acc: 0.7755\n",
            "Epoch 59/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.8861 - acc: 0.7762\n",
            "Epoch 60/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.8772 - acc: 0.7783\n",
            "Epoch 61/300\n",
            "634695/634695 [==============================] - 48s 76us/step - loss: 0.8723 - acc: 0.7796\n",
            "Epoch 62/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.8715 - acc: 0.7793\n",
            "Epoch 63/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.8618 - acc: 0.7819\n",
            "Epoch 64/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.8560 - acc: 0.7836\n",
            "Epoch 65/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.8523 - acc: 0.7840\n",
            "Epoch 66/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.8452 - acc: 0.7857\n",
            "Epoch 67/300\n",
            "634695/634695 [==============================] - 49s 76us/step - loss: 0.8360 - acc: 0.7881\n",
            "Epoch 68/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.8299 - acc: 0.7897\n",
            "Epoch 69/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.8235 - acc: 0.7915\n",
            "Epoch 70/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.8336 - acc: 0.7881\n",
            "Epoch 71/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.8218 - acc: 0.7913\n",
            "Epoch 72/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.8217 - acc: 0.7907\n",
            "Epoch 73/300\n",
            "634695/634695 [==============================] - 49s 76us/step - loss: 0.8124 - acc: 0.7936\n",
            "Epoch 74/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7998 - acc: 0.7968\n",
            "Epoch 75/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7921 - acc: 0.7991\n",
            "Epoch 76/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.8001 - acc: 0.7960\n",
            "Epoch 77/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7902 - acc: 0.7988\n",
            "Epoch 78/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7812 - acc: 0.8017\n",
            "Epoch 79/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7730 - acc: 0.8036\n",
            "Epoch 80/300\n",
            "634695/634695 [==============================] - 48s 76us/step - loss: 0.7671 - acc: 0.8051\n",
            "Epoch 81/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7686 - acc: 0.8043\n",
            "Epoch 82/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7663 - acc: 0.8045\n",
            "Epoch 83/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7660 - acc: 0.8049\n",
            "Epoch 84/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7630 - acc: 0.8052\n",
            "Epoch 85/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7537 - acc: 0.8078\n",
            "Epoch 86/300\n",
            "634695/634695 [==============================] - 49s 76us/step - loss: 0.7494 - acc: 0.8091\n",
            "Epoch 87/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7466 - acc: 0.8095\n",
            "Epoch 88/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7423 - acc: 0.8107\n",
            "Epoch 89/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7376 - acc: 0.8120\n",
            "Epoch 90/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7351 - acc: 0.8125\n",
            "Epoch 91/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7321 - acc: 0.8131\n",
            "Epoch 92/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7286 - acc: 0.8138\n",
            "Epoch 93/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7292 - acc: 0.8133\n",
            "Epoch 94/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7282 - acc: 0.8137\n",
            "Epoch 95/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7222 - acc: 0.8153\n",
            "Epoch 96/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7202 - acc: 0.8158\n",
            "Epoch 97/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7151 - acc: 0.8173\n",
            "Epoch 98/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7157 - acc: 0.8167\n",
            "Epoch 99/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7144 - acc: 0.8170\n",
            "Epoch 100/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7171 - acc: 0.8160\n",
            "Epoch 101/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7179 - acc: 0.8152\n",
            "Epoch 102/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7069 - acc: 0.8188\n",
            "Epoch 103/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6994 - acc: 0.8208\n",
            "Epoch 104/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7017 - acc: 0.8200\n",
            "Epoch 105/300\n",
            "634695/634695 [==============================] - 49s 76us/step - loss: 0.7226 - acc: 0.8131\n",
            "Epoch 106/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.7058 - acc: 0.8183\n",
            "Epoch 107/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6912 - acc: 0.8227\n",
            "Epoch 108/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6787 - acc: 0.8266\n",
            "Epoch 109/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6727 - acc: 0.8280\n",
            "Epoch 110/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6786 - acc: 0.8258\n",
            "Epoch 111/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6808 - acc: 0.8251\n",
            "Epoch 112/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6762 - acc: 0.8267\n",
            "Epoch 113/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6695 - acc: 0.8281\n",
            "Epoch 114/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6659 - acc: 0.8291\n",
            "Epoch 115/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6698 - acc: 0.8279\n",
            "Epoch 116/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6711 - acc: 0.8273\n",
            "Epoch 117/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6651 - acc: 0.8292\n",
            "Epoch 118/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6585 - acc: 0.8314\n",
            "Epoch 119/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6590 - acc: 0.8308\n",
            "Epoch 120/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6554 - acc: 0.8316\n",
            "Epoch 121/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6550 - acc: 0.8317\n",
            "Epoch 122/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6496 - acc: 0.8333\n",
            "Epoch 123/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6514 - acc: 0.8323\n",
            "Epoch 124/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6521 - acc: 0.8321\n",
            "Epoch 125/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6433 - acc: 0.8348\n",
            "Epoch 126/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6443 - acc: 0.8343\n",
            "Epoch 127/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6395 - acc: 0.8357\n",
            "Epoch 128/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6373 - acc: 0.8365\n",
            "Epoch 129/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6340 - acc: 0.8371\n",
            "Epoch 130/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6394 - acc: 0.8355\n",
            "Epoch 131/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6439 - acc: 0.8335\n",
            "Epoch 132/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6451 - acc: 0.8333\n",
            "Epoch 133/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6411 - acc: 0.8341\n",
            "Epoch 134/300\n",
            "634695/634695 [==============================] - 49s 78us/step - loss: 0.6290 - acc: 0.8382\n",
            "Epoch 135/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6221 - acc: 0.8402\n",
            "Epoch 136/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6180 - acc: 0.8412\n",
            "Epoch 137/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6158 - acc: 0.8415\n",
            "Epoch 138/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6150 - acc: 0.8419\n",
            "Epoch 139/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6169 - acc: 0.8411\n",
            "Epoch 140/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6428 - acc: 0.8329\n",
            "Epoch 141/300\n",
            "634695/634695 [==============================] - 49s 78us/step - loss: 0.6444 - acc: 0.8324\n",
            "Epoch 142/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6385 - acc: 0.8341\n",
            "Epoch 143/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6308 - acc: 0.8370\n",
            "Epoch 144/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6220 - acc: 0.8395\n",
            "Epoch 145/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6078 - acc: 0.8441\n",
            "Epoch 146/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6039 - acc: 0.8448\n",
            "Epoch 147/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.6016 - acc: 0.8454\n",
            "Epoch 148/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5994 - acc: 0.8459\n",
            "Epoch 149/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5939 - acc: 0.8475\n",
            "Epoch 150/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5887 - acc: 0.8490\n",
            "Epoch 151/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5863 - acc: 0.8495\n",
            "Epoch 152/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5903 - acc: 0.8480\n",
            "Epoch 153/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5896 - acc: 0.8483\n",
            "Epoch 154/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5880 - acc: 0.8484\n",
            "Epoch 155/300\n",
            "634695/634695 [==============================] - 49s 78us/step - loss: 0.5904 - acc: 0.8476\n",
            "Epoch 156/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5930 - acc: 0.8468\n",
            "Epoch 157/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5916 - acc: 0.8472\n",
            "Epoch 158/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5950 - acc: 0.8459\n",
            "Epoch 159/300\n",
            "634695/634695 [==============================] - 49s 78us/step - loss: 0.5970 - acc: 0.8455\n",
            "Epoch 160/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5992 - acc: 0.8447\n",
            "Epoch 161/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5945 - acc: 0.8462\n",
            "Epoch 162/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5901 - acc: 0.8474\n",
            "Epoch 163/300\n",
            "634695/634695 [==============================] - 49s 78us/step - loss: 0.5871 - acc: 0.8481\n",
            "Epoch 164/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5925 - acc: 0.8464\n",
            "Epoch 165/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5858 - acc: 0.8483\n",
            "Epoch 166/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5833 - acc: 0.8491\n",
            "Epoch 167/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5813 - acc: 0.8493\n",
            "Epoch 168/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5819 - acc: 0.8493\n",
            "Epoch 169/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5796 - acc: 0.8495\n",
            "Epoch 170/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5744 - acc: 0.8512\n",
            "Epoch 171/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5727 - acc: 0.8518\n",
            "Epoch 172/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5757 - acc: 0.8508\n",
            "Epoch 173/300\n",
            "634695/634695 [==============================] - 49s 78us/step - loss: 0.5732 - acc: 0.8511\n",
            "Epoch 174/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5708 - acc: 0.8525\n",
            "Epoch 175/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5711 - acc: 0.8522\n",
            "Epoch 176/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5740 - acc: 0.8508\n",
            "Epoch 177/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5706 - acc: 0.8522\n",
            "Epoch 178/300\n",
            "634695/634695 [==============================] - 49s 78us/step - loss: 0.5689 - acc: 0.8524\n",
            "Epoch 179/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5703 - acc: 0.8516\n",
            "Epoch 180/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5717 - acc: 0.8513\n",
            "Epoch 181/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5623 - acc: 0.8546\n",
            "Epoch 182/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5553 - acc: 0.8568\n",
            "Epoch 183/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5552 - acc: 0.8565\n",
            "Epoch 184/300\n",
            "634695/634695 [==============================] - 49s 78us/step - loss: 0.5662 - acc: 0.8529\n",
            "Epoch 185/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5645 - acc: 0.8530\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5674 - acc: 0.8523\n",
            "Epoch 187/300\n",
            "634695/634695 [==============================] - 49s 78us/step - loss: 0.5654 - acc: 0.8528\n",
            "Epoch 188/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5697 - acc: 0.8514\n",
            "Epoch 189/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5680 - acc: 0.8519\n",
            "Epoch 190/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5633 - acc: 0.8532\n",
            "Epoch 191/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5532 - acc: 0.8567\n",
            "Epoch 192/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5442 - acc: 0.8592\n",
            "Epoch 193/300\n",
            "634695/634695 [==============================] - 49s 78us/step - loss: 0.5407 - acc: 0.8603\n",
            "Epoch 194/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5440 - acc: 0.8589\n",
            "Epoch 195/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5507 - acc: 0.8568\n",
            "Epoch 196/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5484 - acc: 0.8573\n",
            "Epoch 197/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5435 - acc: 0.8587\n",
            "Epoch 198/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5572 - acc: 0.8543\n",
            "Epoch 199/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5657 - acc: 0.8514\n",
            "Epoch 200/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5521 - acc: 0.8561\n",
            "Epoch 201/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5473 - acc: 0.8573\n",
            "Epoch 202/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5439 - acc: 0.8588\n",
            "Epoch 203/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5380 - acc: 0.8605\n",
            "Epoch 204/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5407 - acc: 0.8592\n",
            "Epoch 205/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5410 - acc: 0.8593\n",
            "Epoch 206/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5419 - acc: 0.8590\n",
            "Epoch 207/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5406 - acc: 0.8594\n",
            "Epoch 208/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5361 - acc: 0.8604\n",
            "Epoch 209/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5342 - acc: 0.8612\n",
            "Epoch 210/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5331 - acc: 0.8612\n",
            "Epoch 211/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5287 - acc: 0.8626\n",
            "Epoch 212/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5255 - acc: 0.8635\n",
            "Epoch 213/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5343 - acc: 0.8605\n",
            "Epoch 214/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5364 - acc: 0.8601\n",
            "Epoch 215/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5294 - acc: 0.8623\n",
            "Epoch 216/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5239 - acc: 0.8636\n",
            "Epoch 217/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5224 - acc: 0.8642\n",
            "Epoch 218/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5263 - acc: 0.8629\n",
            "Epoch 219/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5276 - acc: 0.8624\n",
            "Epoch 220/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5283 - acc: 0.8620\n",
            "Epoch 221/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5293 - acc: 0.8617\n",
            "Epoch 222/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5276 - acc: 0.8624\n",
            "Epoch 223/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5251 - acc: 0.8631\n",
            "Epoch 224/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5243 - acc: 0.8632\n",
            "Epoch 225/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5344 - acc: 0.8600\n",
            "Epoch 226/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5403 - acc: 0.8580\n",
            "Epoch 227/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5415 - acc: 0.8575\n",
            "Epoch 228/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5389 - acc: 0.8583\n",
            "Epoch 229/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5368 - acc: 0.8592\n",
            "Epoch 230/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5295 - acc: 0.8614\n",
            "Epoch 231/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5318 - acc: 0.8600\n",
            "Epoch 232/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5261 - acc: 0.8622\n",
            "Epoch 233/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5284 - acc: 0.8613\n",
            "Epoch 234/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5192 - acc: 0.8644\n",
            "Epoch 235/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5171 - acc: 0.8650\n",
            "Epoch 236/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5183 - acc: 0.8646\n",
            "Epoch 237/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5115 - acc: 0.8668\n",
            "Epoch 238/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5070 - acc: 0.8681\n",
            "Epoch 239/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5055 - acc: 0.8683\n",
            "Epoch 240/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5029 - acc: 0.8693\n",
            "Epoch 241/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5038 - acc: 0.8687\n",
            "Epoch 242/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5029 - acc: 0.8688\n",
            "Epoch 243/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4992 - acc: 0.8699\n",
            "Epoch 244/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5014 - acc: 0.8692\n",
            "Epoch 245/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5108 - acc: 0.8662\n",
            "Epoch 246/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5114 - acc: 0.8661\n",
            "Epoch 247/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5174 - acc: 0.8638\n",
            "Epoch 248/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5201 - acc: 0.8631\n",
            "Epoch 249/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5247 - acc: 0.8613\n",
            "Epoch 250/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5177 - acc: 0.8639\n",
            "Epoch 251/300\n",
            "634695/634695 [==============================] - 49s 76us/step - loss: 0.5174 - acc: 0.8637\n",
            "Epoch 252/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5116 - acc: 0.8657\n",
            "Epoch 253/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5091 - acc: 0.8663\n",
            "Epoch 254/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4992 - acc: 0.8696\n",
            "Epoch 255/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4922 - acc: 0.8719\n",
            "Epoch 256/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4927 - acc: 0.8713\n",
            "Epoch 257/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4882 - acc: 0.8728\n",
            "Epoch 258/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4890 - acc: 0.8723\n",
            "Epoch 259/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4852 - acc: 0.8734\n",
            "Epoch 260/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4876 - acc: 0.8728\n",
            "Epoch 261/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4938 - acc: 0.8705\n",
            "Epoch 262/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5038 - acc: 0.8670\n",
            "Epoch 263/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5185 - acc: 0.8626\n",
            "Epoch 264/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5170 - acc: 0.8633\n",
            "Epoch 265/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5217 - acc: 0.8617\n",
            "Epoch 266/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5201 - acc: 0.8622\n",
            "Epoch 267/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5145 - acc: 0.8641\n",
            "Epoch 268/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5047 - acc: 0.8674\n",
            "Epoch 269/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5048 - acc: 0.8672\n",
            "Epoch 270/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5013 - acc: 0.8681\n",
            "Epoch 271/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4915 - acc: 0.8714\n",
            "Epoch 272/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4920 - acc: 0.8709\n",
            "Epoch 273/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4894 - acc: 0.8717\n",
            "Epoch 274/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4913 - acc: 0.8712\n",
            "Epoch 275/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5077 - acc: 0.8654\n",
            "Epoch 276/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4987 - acc: 0.8685\n",
            "Epoch 277/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5189 - acc: 0.8619\n",
            "Epoch 278/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5242 - acc: 0.8601\n",
            "Epoch 279/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4993 - acc: 0.8681\n",
            "Epoch 280/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4854 - acc: 0.8729\n",
            "Epoch 281/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5122 - acc: 0.8648\n",
            "Epoch 282/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5321 - acc: 0.8578\n",
            "Epoch 283/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5221 - acc: 0.8608\n",
            "Epoch 284/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.5065 - acc: 0.8659\n",
            "Epoch 285/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4976 - acc: 0.8690\n",
            "Epoch 286/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4881 - acc: 0.8718\n",
            "Epoch 287/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4782 - acc: 0.8752\n",
            "Epoch 288/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4694 - acc: 0.8777\n",
            "Epoch 289/300\n",
            "634695/634695 [==============================] - 49s 76us/step - loss: 0.4655 - acc: 0.8787\n",
            "Epoch 290/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4625 - acc: 0.8798\n",
            "Epoch 291/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4624 - acc: 0.8798\n",
            "Epoch 292/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4617 - acc: 0.8797\n",
            "Epoch 293/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4600 - acc: 0.8800\n",
            "Epoch 294/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4613 - acc: 0.8795\n",
            "Epoch 295/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4616 - acc: 0.8793\n",
            "Epoch 296/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4775 - acc: 0.8741\n",
            "Epoch 297/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4850 - acc: 0.8716\n",
            "Epoch 298/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4880 - acc: 0.8706\n",
            "Epoch 299/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4862 - acc: 0.8716\n",
            "Epoch 300/300\n",
            "634695/634695 [==============================] - 49s 77us/step - loss: 0.4860 - acc: 0.8715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f62c2411550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYkf-2nOXUtu",
        "colab_type": "code",
        "outputId": "a23520b2-7ef6-44a9-b16b-ece8c637e848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "lstm_model.evaluate(test_tokens, test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70522/70522 [==============================] - 50s 709us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7.0228537029296545, 0.37249368991321297]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii8l32o1XUuC",
        "colab_type": "text"
      },
      "source": [
        "## Comparing our models\n",
        "\n",
        "Let's compare the predictions from our models on a sample of our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkfA0mrpXUuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_char_vectors, _ = next(data_generator(test_tweets, None)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSte0GpgXUuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = {\n",
        "    label: [emojis[np.argmax(x)] for x in pred]\n",
        "    for label, pred in (\n",
        "        ('lstm', lstm_model.predict(test_tokens[:100])),\n",
        "        ('cnn', cnn_model.predict(test_tokens[:100])),\n",
        "    )\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyhnitJQXUuM",
        "colab_type": "code",
        "outputId": "7c89b426-95af-4200-c725-95bcf861556e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        }
      },
      "source": [
        "# Make a dataframe just for test data\n",
        "pd.options.display.max_colwidth = 128\n",
        "test_df = test_tweets[:100].reset_index()\n",
        "eval_df = pd.DataFrame({\n",
        "    'content': test_df['text'],\n",
        "    'true': test_df['emoji'],\n",
        "    **predictions\n",
        "})\n",
        "eval_df[['content', 'true', 'cnn', 'lstm']].head(25)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>true</th>\n",
              "      <th>cnn</th>\n",
              "      <th>lstm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@iAmydear: Done watching this episode. Mix emotions, but dominant is understanding and at peace. #ALDUBBlessedPair</td>\n",
              "      <td>‚ù§</td>\n",
              "      <td>‚ù§</td>\n",
              "      <td>‚ù§</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Congrats on SNL you and Tyler never fail to shooken me :)</td>\n",
              "      <td>‚ú®</td>\n",
              "      <td>‚úå</td>\n",
              "      <td>‚ù§</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@iiBreakNecks: dude playing dramatic music in the background while Cam'ron arguing with his girl. LEGENDARY</td>\n",
              "      <td>üòÇ</td>\n",
              "      <td>üòÇ</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>All these denial letters I been getting</td>\n",
              "      <td>üò©</td>\n",
              "      <td>üòê</td>\n",
              "      <td>üíÄ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@LeanOverHoes: You mad bitch well stay mad .</td>\n",
              "      <td>üòÇ</td>\n",
              "      <td>üòÇ</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>@AvianaMonjae: A boogie x friend zone</td>\n",
              "      <td>üòÑ</td>\n",
              "      <td>‚ù§</td>\n",
              "      <td>‚ù§</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>@zenkmm: We fight,kiss,hug,text,argue,laugh,smile,love...that's us #ALDUBLoveEndures @DJGraphicsArts @fleurs7754 @xinempl</td>\n",
              "      <td>‚ù§</td>\n",
              "      <td>‚ò∫</td>\n",
              "      <td>‚ù§</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>LunaLaCreme \"I officially would not be able to go to Victoria, Australia  \"</td>\n",
              "      <td>üòÜ</td>\n",
              "      <td>üòã</td>\n",
              "      <td>üíÄ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>@Maelle1505 @selijedhead @Nele_x3_ @lauraJEDsmiley Hope we can have a reunion soooooooon!</td>\n",
              "      <td>üíñ</td>\n",
              "      <td>üíú</td>\n",
              "      <td>üíï</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>@GagaMagazineUK: V Magazine have been loving Lady Gaga over the past few days</td>\n",
              "      <td>üëÄ</td>\n",
              "      <td>üòç</td>\n",
              "      <td>üòö</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Fresh cut who this</td>\n",
              "      <td>üòé</td>\n",
              "      <td>‚ù§</td>\n",
              "      <td>üòé</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>@Allykat728 oh yea and the people walking in and out of the woods in vests near by too</td>\n",
              "      <td>üò≠</td>\n",
              "      <td>üòé</td>\n",
              "      <td>üôå</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>@danaenicole__: oh yes</td>\n",
              "      <td>üòç</td>\n",
              "      <td>üòç</td>\n",
              "      <td>üòç</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Hoh 80 deirr its peanuts</td>\n",
              "      <td>üòÇ</td>\n",
              "      <td>üò≥</td>\n",
              "      <td>üò≠</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Let's flirt on webcam O &lt;== Just find me there</td>\n",
              "      <td>‚è≠</td>\n",
              "      <td>‚û°</td>\n",
              "      <td>üëâ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>I'M D E V A S T A T E D</td>\n",
              "      <td>üò≠</td>\n",
              "      <td>üôÑ</td>\n",
              "      <td>üò≠</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>@urbandoll: RT to win: Kylie Lip Kit of your choice must be following me and @BeautyyIcon</td>\n",
              "      <td>üíó</td>\n",
              "      <td>üíó</td>\n",
              "      <td>üíó</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>@cleoh_da: I will die if you ever leave Me.... 10 year later she is still alive eating ugali #100liesmyextoldme</td>\n",
              "      <td>üòÇ</td>\n",
              "      <td>üòí</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>I DID!!!</td>\n",
              "      <td>üòä</td>\n",
              "      <td>üòÇ</td>\n",
              "      <td>üò≠</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>I use to hang clown pictures outside of my locker in 7th grade. @sav_taylor18 hated it look at where we are now in the world</td>\n",
              "      <td>üòÇ</td>\n",
              "      <td>üòÇ</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>@dangerouzdallas @ShawnMendes aww this is adorable</td>\n",
              "      <td>‚ù§</td>\n",
              "      <td>‚ù§</td>\n",
              "      <td>üòç</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>I have even better days coming</td>\n",
              "      <td>üòè</td>\n",
              "      <td>üòÇ</td>\n",
              "      <td>üòï</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>@Trevor_Dc @DruggieFeels thats really how you be bih</td>\n",
              "      <td>üòÇ</td>\n",
              "      <td>‚úå</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>@luluheartfillia: Our happy little family #nalu</td>\n",
              "      <td>üòÇ</td>\n",
              "      <td>‚ù§</td>\n",
              "      <td>‚ù§</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>@WhatsTheIssue__: I DONT Give Seconds Chances</td>\n",
              "      <td>‚Äº</td>\n",
              "      <td>üôÑ</td>\n",
              "      <td>üò¥</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                         content  \\\n",
              "0             @iAmydear: Done watching this episode. Mix emotions, but dominant is understanding and at peace. #ALDUBBlessedPair   \n",
              "1                                                                      Congrats on SNL you and Tyler never fail to shooken me :)   \n",
              "2                    @iiBreakNecks: dude playing dramatic music in the background while Cam'ron arguing with his girl. LEGENDARY   \n",
              "3                                                                                        All these denial letters I been getting   \n",
              "4                                                                                   @LeanOverHoes: You mad bitch well stay mad .   \n",
              "5                                                                                          @AvianaMonjae: A boogie x friend zone   \n",
              "6      @zenkmm: We fight,kiss,hug,text,argue,laugh,smile,love...that's us #ALDUBLoveEndures @DJGraphicsArts @fleurs7754 @xinempl   \n",
              "7                                                    LunaLaCreme \"I officially would not be able to go to Victoria, Australia  \"   \n",
              "8                                      @Maelle1505 @selijedhead @Nele_x3_ @lauraJEDsmiley Hope we can have a reunion soooooooon!   \n",
              "9                                                  @GagaMagazineUK: V Magazine have been loving Lady Gaga over the past few days   \n",
              "10                                                                                                            Fresh cut who this   \n",
              "11                                        @Allykat728 oh yea and the people walking in and out of the woods in vests near by too   \n",
              "12                                                                                                        @danaenicole__: oh yes   \n",
              "13                                                                                                      Hoh 80 deirr its peanuts   \n",
              "14                                                                                Let's flirt on webcam O <== Just find me there   \n",
              "15                                                                                                       I'M D E V A S T A T E D   \n",
              "16                                     @urbandoll: RT to win: Kylie Lip Kit of your choice must be following me and @BeautyyIcon   \n",
              "17               @cleoh_da: I will die if you ever leave Me.... 10 year later she is still alive eating ugali #100liesmyextoldme   \n",
              "18                                                                                                                      I DID!!!   \n",
              "19  I use to hang clown pictures outside of my locker in 7th grade. @sav_taylor18 hated it look at where we are now in the world   \n",
              "20                                                                            @dangerouzdallas @ShawnMendes aww this is adorable   \n",
              "21                                                                                                I have even better days coming   \n",
              "22                                                                          @Trevor_Dc @DruggieFeels thats really how you be bih   \n",
              "23                                                                               @luluheartfillia: Our happy little family #nalu   \n",
              "24                                                                                 @WhatsTheIssue__: I DONT Give Seconds Chances   \n",
              "\n",
              "   true cnn lstm  \n",
              "0     ‚ù§   ‚ù§    ‚ù§  \n",
              "1     ‚ú®   ‚úå    ‚ù§  \n",
              "2     üòÇ   üòÇ    üòÇ  \n",
              "3     üò©   üòê    üíÄ  \n",
              "4     üòÇ   üòÇ    üòÇ  \n",
              "5     üòÑ   ‚ù§    ‚ù§  \n",
              "6     ‚ù§   ‚ò∫    ‚ù§  \n",
              "7     üòÜ   üòã    üíÄ  \n",
              "8     üíñ   üíú    üíï  \n",
              "9     üëÄ   üòç    üòö  \n",
              "10    üòé   ‚ù§    üòé  \n",
              "11    üò≠   üòé    üôå  \n",
              "12    üòç   üòç    üòç  \n",
              "13    üòÇ   üò≥    üò≠  \n",
              "14    ‚è≠   ‚û°    üëâ  \n",
              "15    üò≠   üôÑ    üò≠  \n",
              "16    üíó   üíó    üíó  \n",
              "17    üòÇ   üòí    üòÇ  \n",
              "18    üòä   üòÇ    üò≠  \n",
              "19    üòÇ   üòÇ    üòÇ  \n",
              "20    ‚ù§   ‚ù§    üòç  \n",
              "21    üòè   üòÇ    üòï  \n",
              "22    üòÇ   ‚úå    üòÇ  \n",
              "23    üòÇ   ‚ù§    ‚ù§  \n",
              "24    ‚Äº   üôÑ    üò¥  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_WWohnyXUuQ",
        "colab_type": "text"
      },
      "source": [
        "## Qualitative Evaluation\n",
        "\n",
        "We can examine some of our error cases by hand.  Often, the models tend to agree when they make mistakes, and that the mistakes aren't unreasonable: this task would be challenging even for a human."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRtGsuBiXUuS",
        "colab_type": "code",
        "outputId": "dc5f11f1-c5eb-4f03-9c54-1091a87d9962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "eval_df[eval_df['lstm'] != eval_df['true']].head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cnn</th>\n",
              "      <th>content</th>\n",
              "      <th>lstm</th>\n",
              "      <th>true</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>‚úå</td>\n",
              "      <td>Congrats on SNL you and Tyler never fail to shooken me :)</td>\n",
              "      <td>‚ù§</td>\n",
              "      <td>‚ú®</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>üòê</td>\n",
              "      <td>All these denial letters I been getting</td>\n",
              "      <td>üíÄ</td>\n",
              "      <td>üò©</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>‚ù§</td>\n",
              "      <td>@AvianaMonjae: A boogie x friend zone</td>\n",
              "      <td>‚ù§</td>\n",
              "      <td>üòÑ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>üòã</td>\n",
              "      <td>LunaLaCreme \"I officially would not be able to go to Victoria, Australia  \"</td>\n",
              "      <td>üíÄ</td>\n",
              "      <td>üòÜ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>üíú</td>\n",
              "      <td>@Maelle1505 @selijedhead @Nele_x3_ @lauraJEDsmiley Hope we can have a reunion soooooooon!</td>\n",
              "      <td>üíï</td>\n",
              "      <td>üíñ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>üòç</td>\n",
              "      <td>@GagaMagazineUK: V Magazine have been loving Lady Gaga over the past few days</td>\n",
              "      <td>üòö</td>\n",
              "      <td>üëÄ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>üòé</td>\n",
              "      <td>@Allykat728 oh yea and the people walking in and out of the woods in vests near by too</td>\n",
              "      <td>üôå</td>\n",
              "      <td>üò≠</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>üò≥</td>\n",
              "      <td>Hoh 80 deirr its peanuts</td>\n",
              "      <td>üò≠</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>‚û°</td>\n",
              "      <td>Let's flirt on webcam O &lt;== Just find me there</td>\n",
              "      <td>üëâ</td>\n",
              "      <td>‚è≠</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>üòÇ</td>\n",
              "      <td>I DID!!!</td>\n",
              "      <td>üò≠</td>\n",
              "      <td>üòä</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   cnn  \\\n",
              "1    ‚úå   \n",
              "3    üòê   \n",
              "5    ‚ù§   \n",
              "7    üòã   \n",
              "8    üíú   \n",
              "9    üòç   \n",
              "11   üòé   \n",
              "13   üò≥   \n",
              "14   ‚û°   \n",
              "18   üòÇ   \n",
              "\n",
              "                                                                                      content  \\\n",
              "1                                   Congrats on SNL you and Tyler never fail to shooken me :)   \n",
              "3                                                     All these denial letters I been getting   \n",
              "5                                                       @AvianaMonjae: A boogie x friend zone   \n",
              "7                 LunaLaCreme \"I officially would not be able to go to Victoria, Australia  \"   \n",
              "8   @Maelle1505 @selijedhead @Nele_x3_ @lauraJEDsmiley Hope we can have a reunion soooooooon!   \n",
              "9               @GagaMagazineUK: V Magazine have been loving Lady Gaga over the past few days   \n",
              "11     @Allykat728 oh yea and the people walking in and out of the woods in vests near by too   \n",
              "13                                                                   Hoh 80 deirr its peanuts   \n",
              "14                                             Let's flirt on webcam O <== Just find me there   \n",
              "18                                                                                   I DID!!!   \n",
              "\n",
              "   lstm true  \n",
              "1     ‚ù§    ‚ú®  \n",
              "3     üíÄ    üò©  \n",
              "5     ‚ù§    üòÑ  \n",
              "7     üíÄ    üòÜ  \n",
              "8     üíï    üíñ  \n",
              "9     üòö    üëÄ  \n",
              "11    üôå    üò≠  \n",
              "13    üò≠    üòÇ  \n",
              "14    üëâ    ‚è≠  \n",
              "18    üò≠    üòä  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26qzLmeQXUuW",
        "colab_type": "code",
        "outputId": "aef8179e-a8f4-4bb7-b521-d890880d24d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def combined_data_generator(tweets, tokens, batch_size):\n",
        "    tweets = tweets.reset_index()\n",
        "    while True:\n",
        "        batch_idx = random.sample(range(len(tweets)), batch_size)\n",
        "        tweet_batch = tweets.iloc[batch_idx]\n",
        "        token_batch = tokens[batch_idx]\n",
        "        char_vec = np.zeros((batch_size, max_sequence_len, len(chars)))\n",
        "        token_vec = np.zeros((batch_size, max_num_tokens))\n",
        "        y = np.zeros((batch_size,))\n",
        "        for row_idx, (token_row, (_, tweet_row)) in enumerate(zip(token_batch, tweet_batch.iterrows())):\n",
        "            y[row_idx] = emoji_to_idx[tweet_row['emoji']]\n",
        "            for ch_idx, ch in enumerate(tweet_row['text']):\n",
        "                char_vec[row_idx, ch_idx, char_to_idx[ch]] = 1\n",
        "            token_vec[row_idx, :] = token_row\n",
        "        yield { 'cnn_input': token_vec, 'lstm_input': token_vec}, y\n",
        "\n",
        "d, y = next(combined_data_generator(train_tweets, training_tokens, 5))\n",
        "d['lstm_input'].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 54)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vdTIRA8XUuc",
        "colab_type": "code",
        "outputId": "7a7da14d-2e40-49d8-b93f-92443bda2202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        }
      },
      "source": [
        "def prediction_layer(model):\n",
        "    layers = [layer for layer in model.layers if layer.name.endswith('_predictions')]\n",
        "    return layers[0].output\n",
        "\n",
        "def create_ensemble(*models):\n",
        "    inputs = [model.input for model in models]\n",
        "    predictions = [prediction_layer(model) for model in models]\n",
        "    merged = Average()(predictions)\n",
        "    model = Model(\n",
        "        inputs=inputs,\n",
        "        outputs=[merged],\n",
        "    )\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "ensemble = create_ensemble(cnn_model, lstm_model)\n",
        "ensemble.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cnn_input (InputLayer)          (None, 54)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "cnn_embedding (Embedding)       (None, 54, 100)      10000000    cnn_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 53, 128)      25728       cnn_embedding[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 52, 128)      38528       cnn_embedding[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 26, 128)      0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 26, 128)      0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 25, 256)      65792       max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 24, 256)      98560       max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 12, 256)      0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 12, 256)      0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 11, 256)      131328      max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 10, 256)      196864      max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 256)          0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 256)          0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_input (InputLayer)         (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 512)          0           global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_max_pooling1d_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_embedding (Embedding)      (None, None, 100)    10000000    lstm_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          65664       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 128)          117248      lstm_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "cnn_predictions (Dense)         (None, 121)          15609       dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_predictions (Dense)        (None, 121)          15609       lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "average_1 (Average)             (None, 121)          0           cnn_predictions[0][0]            \n",
            "                                                                 lstm_predictions[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 20,770,930\n",
            "Trainable params: 20,770,930\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOsus1zCXUuv",
        "colab_type": "code",
        "outputId": "f159eb7b-da65-4bba-e124-cb33ae35637d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BATCH_SIZE = 10000\n",
        "ensemble.fit_generator(\n",
        "    combined_data_generator(train_tweets, training_tokens, BATCH_SIZE),\n",
        "    epochs=100,\n",
        "    steps_per_epoch=len(train_tweets) / BATCH_SIZE,\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO8AxYBKXUu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ensemble.evaluate_generator(\n",
        "    combined_data_generator(test_tweets, test_tokens, BATCH_SIZE),\n",
        "    steps=len(test_tweets) / BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht3D2DN1XUu-",
        "colab_type": "code",
        "outputId": "cbb9c563-3453-47fc-e497-bee9b8963535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_tweets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "634695"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTLOxuEkXUvC",
        "colab_type": "code",
        "outputId": "a61dd193-bccb-45fe-f8ef-375144fc9631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1882
        }
      },
      "source": [
        "data = [[\"I love you\"],[\"superb\"],[\"hey\"],[\"hello\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],\n",
        "       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],[\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],\n",
        "       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],[\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],\n",
        "       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],\n",
        "       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],\n",
        "       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],\n",
        "       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],\n",
        "       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],\n",
        "       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],\n",
        "       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],\n",
        "       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],\n",
        "       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],\n",
        "       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],       [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],\n",
        "       [\"Thank you for dinner!\"],      [\"Thank you for dinner!\"],[\"I don't like it\"],[\"My car skidded on the wet street\"],\n",
        "       ]\n",
        "df = pd.DataFrame(data,columns=['text'])\n",
        "(df !='n').astype(int)\n",
        "df\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I love you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>superb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hello</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>I don't like it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows √ó 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                text\n",
              "0                         I love you\n",
              "1                             superb\n",
              "2                                hey\n",
              "3                              hello\n",
              "4                    I don't like it\n",
              "5   My car skidded on the wet street\n",
              "6              Thank you for dinner!\n",
              "7                    I don't like it\n",
              "8   My car skidded on the wet street\n",
              "9              Thank you for dinner!\n",
              "10                   I don't like it\n",
              "11  My car skidded on the wet street\n",
              "12             Thank you for dinner!\n",
              "13                   I don't like it\n",
              "14  My car skidded on the wet street\n",
              "15             Thank you for dinner!\n",
              "16                   I don't like it\n",
              "17  My car skidded on the wet street\n",
              "18             Thank you for dinner!\n",
              "19                   I don't like it\n",
              "20  My car skidded on the wet street\n",
              "21             Thank you for dinner!\n",
              "22                   I don't like it\n",
              "23  My car skidded on the wet street\n",
              "24             Thank you for dinner!\n",
              "25                   I don't like it\n",
              "26  My car skidded on the wet street\n",
              "27             Thank you for dinner!\n",
              "28                   I don't like it\n",
              "29  My car skidded on the wet street\n",
              "..                               ...\n",
              "70                   I don't like it\n",
              "71  My car skidded on the wet street\n",
              "72             Thank you for dinner!\n",
              "73                   I don't like it\n",
              "74  My car skidded on the wet street\n",
              "75             Thank you for dinner!\n",
              "76                   I don't like it\n",
              "77  My car skidded on the wet street\n",
              "78             Thank you for dinner!\n",
              "79                   I don't like it\n",
              "80  My car skidded on the wet street\n",
              "81             Thank you for dinner!\n",
              "82                   I don't like it\n",
              "83  My car skidded on the wet street\n",
              "84             Thank you for dinner!\n",
              "85                   I don't like it\n",
              "86  My car skidded on the wet street\n",
              "87             Thank you for dinner!\n",
              "88                   I don't like it\n",
              "89  My car skidded on the wet street\n",
              "90             Thank you for dinner!\n",
              "91                   I don't like it\n",
              "92  My car skidded on the wet street\n",
              "93             Thank you for dinner!\n",
              "94                   I don't like it\n",
              "95  My car skidded on the wet street\n",
              "96             Thank you for dinner!\n",
              "97             Thank you for dinner!\n",
              "98                   I don't like it\n",
              "99  My car skidded on the wet street\n",
              "\n",
              "[100 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OCKS2HGXUvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "df_tokens=tokenizer.texts_to_sequences(df['text'])\n",
        "df_tokens = pad_sequences(df_tokens, maxlen=max_num_tokens)\n",
        "predictions = {\n",
        "label: [emojis[np.argmax(x)] for x in pred]\n",
        "for label, pred in (\n",
        "('cnn_model', cnn_model.predict(df_tokens[:100])),\n",
        ")\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KVi_4-BXUvL",
        "colab_type": "code",
        "outputId": "6b48fad4-af93-4c04-a895-6762ba23bc44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1882
        }
      },
      "source": [
        "pd.options.display.max_colwidth = 128\n",
        "text = df.reset_index()\n",
        "eval_df = pd.DataFrame({\n",
        "    'content': df['text'],\n",
        "    **predictions\n",
        "})\n",
        "eval_df[['content',  'cnn_model']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>cnn_model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I love you</td>\n",
              "      <td>‚ù§</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>superb</td>\n",
              "      <td>üòÄ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hey</td>\n",
              "      <td>üòç</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hello</td>\n",
              "      <td>üòä</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Thank you for dinner!</td>\n",
              "      <td>üòò</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>I don't like it</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>My car skidded on the wet street</td>\n",
              "      <td>üòÇ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                             content cnn_model\n",
              "0                         I love you         ‚ù§\n",
              "1                             superb         üòÄ\n",
              "2                                hey         üòç\n",
              "3                              hello         üòä\n",
              "4                    I don't like it         üòÇ\n",
              "5   My car skidded on the wet street         üòÇ\n",
              "6              Thank you for dinner!         üòò\n",
              "7                    I don't like it         üòÇ\n",
              "8   My car skidded on the wet street         üòÇ\n",
              "9              Thank you for dinner!         üòò\n",
              "10                   I don't like it         üòÇ\n",
              "11  My car skidded on the wet street         üòÇ\n",
              "12             Thank you for dinner!         üòò\n",
              "13                   I don't like it         üòÇ\n",
              "14  My car skidded on the wet street         üòÇ\n",
              "15             Thank you for dinner!         üòò\n",
              "16                   I don't like it         üòÇ\n",
              "17  My car skidded on the wet street         üòÇ\n",
              "18             Thank you for dinner!         üòò\n",
              "19                   I don't like it         üòÇ\n",
              "20  My car skidded on the wet street         üòÇ\n",
              "21             Thank you for dinner!         üòò\n",
              "22                   I don't like it         üòÇ\n",
              "23  My car skidded on the wet street         üòÇ\n",
              "24             Thank you for dinner!         üòò\n",
              "25                   I don't like it         üòÇ\n",
              "26  My car skidded on the wet street         üòÇ\n",
              "27             Thank you for dinner!         üòò\n",
              "28                   I don't like it         üòÇ\n",
              "29  My car skidded on the wet street         üòÇ\n",
              "..                               ...       ...\n",
              "70                   I don't like it         üòÇ\n",
              "71  My car skidded on the wet street         üòÇ\n",
              "72             Thank you for dinner!         üòò\n",
              "73                   I don't like it         üòÇ\n",
              "74  My car skidded on the wet street         üòÇ\n",
              "75             Thank you for dinner!         üòò\n",
              "76                   I don't like it         üòÇ\n",
              "77  My car skidded on the wet street         üòÇ\n",
              "78             Thank you for dinner!         üòò\n",
              "79                   I don't like it         üòÇ\n",
              "80  My car skidded on the wet street         üòÇ\n",
              "81             Thank you for dinner!         üòò\n",
              "82                   I don't like it         üòÇ\n",
              "83  My car skidded on the wet street         üòÇ\n",
              "84             Thank you for dinner!         üòò\n",
              "85                   I don't like it         üòÇ\n",
              "86  My car skidded on the wet street         üòÇ\n",
              "87             Thank you for dinner!         üòò\n",
              "88                   I don't like it         üòÇ\n",
              "89  My car skidded on the wet street         üòÇ\n",
              "90             Thank you for dinner!         üòò\n",
              "91                   I don't like it         üòÇ\n",
              "92  My car skidded on the wet street         üòÇ\n",
              "93             Thank you for dinner!         üòò\n",
              "94                   I don't like it         üòÇ\n",
              "95  My car skidded on the wet street         üòÇ\n",
              "96             Thank you for dinner!         üòò\n",
              "97             Thank you for dinner!         üòò\n",
              "98                   I don't like it         üòÇ\n",
              "99  My car skidded on the wet street         üòÇ\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDCEMoQtaeSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}